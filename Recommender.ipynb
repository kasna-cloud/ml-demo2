{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dEaVsqSgNyQ"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "4FyfuZX-gTKS"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sT8AyHRMNh41"
   },
   "source": [
    "# TensorFlow Recommenders: Quickstart\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/quickstart\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f-reQ11gbLB"
   },
   "source": [
    "In this tutorial, we build a simple matrix factorization model using the [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/100k/) with TFRS. We can use this model to recommend movies for a given user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA00wBE2Ntdm"
   },
   "source": [
    "### Import TFRS\n",
    "\n",
    "First, install and import TFRS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6yzAaM85Z12D"
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-recommenders --user\n",
    "!pip install -q --upgrade tensorflow-datasets --user\n",
    "!pip install -q tqdm --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "n3oYt3R6Nr9l"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import time    # to be used in loop iterations\n",
    "\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "#rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b52b8544f34022af0c28132e7d14d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use tmqdm for cell execution progress bar:\n",
    "# The general syntax is just like the sample bel. In your loop, just wrap the iterable inside the tqdm()\n",
    "\n",
    "# Loop with a progres bar\n",
    "for i in tqdm(range(100)):\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc63977b4a8e4be6ab60fb5859b2f8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Outer Level:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2019bda94c124c57a8a086e6361aca19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inner Level:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb468de3dd0c41e69fff6595d99e6a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inner Level:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# nested loops with progress bar\n",
    "outer_level = list(range(2))\n",
    "inner_level = list(range(100))\n",
    "for _ in tqdm(outer_level, desc='Outer Level'):\n",
    "    for number in tqdm(inner_level, desc='Inner Level'):\n",
    "        time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCxQ1CZcO2wh"
   },
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RcmlxfaPw__0"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>City_Category</th>\n",
       "      <th>Stay_In_Current_City_Years</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Product_Category_1</th>\n",
       "      <th>Product_Category_2</th>\n",
       "      <th>Product_Category_3</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00069042</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00248942</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00087842</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00085442</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000002</td>\n",
       "      <td>P00285442</td>\n",
       "      <td>M</td>\n",
       "      <td>55+</td>\n",
       "      <td>16</td>\n",
       "      <td>C</td>\n",
       "      <td>4+</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Product_ID Gender   Age  Occupation City_Category  \\\n",
       "0  1000001  P00069042      F  0-17          10             A   \n",
       "1  1000001  P00248942      F  0-17          10             A   \n",
       "2  1000001  P00087842      F  0-17          10             A   \n",
       "3  1000001  P00085442      F  0-17          10             A   \n",
       "4  1000002  P00285442      M   55+          16             C   \n",
       "\n",
       "  Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
       "0                          2               0                   3   \n",
       "1                          2               0                   1   \n",
       "2                          2               0                  12   \n",
       "3                          2               0                  12   \n",
       "4                         4+               0                   8   \n",
       "\n",
       "   Product_Category_2  Product_Category_3  Purchase  \n",
       "0                 NaN                 NaN      8370  \n",
       "1                 6.0                14.0     15200  \n",
       "2                 NaN                 NaN      1422  \n",
       "3                14.0                 NaN      1057  \n",
       "4                 NaN                 NaN      7969  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import Black Friday Sales datasets from GCS\n",
    "\n",
    "# connect to gcs\n",
    "client = storage.Client()\n",
    "bucket_name = \"black_friday_datasets\"\n",
    "\n",
    "train_sales_data = \"train_black_friday.csv\"\n",
    "test_sales_data = \"test_black_friday.csv\"\n",
    "concatenated_sales_datasets = \"full_black_friday.csv\"\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# get blobs from bucket\n",
    "blob_train_sale = bucket.get_blob(train_sales_data)\n",
    "blob_test_sale = bucket.get_blob(test_sales_data)\n",
    "blob_concatenated_sales = bucket.get_blob(concatenated_sales_datasets)\n",
    "\n",
    "# save as local files in /datasets\n",
    "filename_train_sale = blob_train_sale.download_to_filename(\"datasets/\" + train_sales_data)\n",
    "filename_test_sale = blob_test_sale.download_to_filename(\"datasets/\" + test_sales_data)\n",
    "filename_content_concatenated_sales = blob_concatenated_sales.download_to_filename(\"datasets/\" + concatenated_sales_datasets)\n",
    "\n",
    "# open one blob to have a look at the data\n",
    "content_train_sale = blob_train_sale.download_as_bytes()\n",
    "\n",
    "black_friday_df = pd.read_csv(BytesIO(content_train_sale))\n",
    "black_friday_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping function\n",
    "def sales_map(user_id, product_id):\n",
    "    return {\"user_id\": user_id, \"product_id\": product_id}\n",
    "\n",
    "\n",
    "# paths to datasets\n",
    "train_csv_path = \"datasets/train_black_friday.csv\"\n",
    "test_csv_path = \"datasets/test_black_friday.csv\"\n",
    "full_black_friday_ds_path = \"datasets/full_black_friday.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert CSV to TensorFlow Datasets and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aIuLZsWvxs81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DS sample:  {'product_id': \"P00069042\", 'user_id': \"1000001\"}\n",
      "Test DS sample:  {'product_id': \"P00128942\", 'user_id': \"1000004\"}\n"
     ]
    }
   ],
   "source": [
    "# #TODO the model could use more features (currently uses only user_id and product_id)\n",
    "\n",
    "sales_train_ds = tf.data.experimental.CsvDataset(\n",
    "    filenames=train_csv_path,\n",
    "    # record_defaults=[tf.constant([1000000], dtype=tf.int32), tf.string],\n",
    "    record_defaults=[tf.string, tf.string],\n",
    "    select_cols=[0, 1],\n",
    "    field_delim=\",\",\n",
    "    header=True)\n",
    "\n",
    "sales_train_ds = sales_train_ds.map(map_func=sales_map)\n",
    "# sales_train_ds = sales_train_ds.batch(1)\n",
    "\n",
    "for data in sales_train_ds:\n",
    "    tf.print(\"Train DS sample: \", data)  # {'product_id': [\"P00069042\"], 'user_id': [1000001]}\n",
    "    break\n",
    "\n",
    "sales_test_ds = tf.data.experimental.CsvDataset(\n",
    "    filenames=test_csv_path,\n",
    "    # record_defaults=[tf.constant([1000000], dtype=tf.int32), tf.string],\n",
    "    record_defaults=[tf.string, tf.string],\n",
    "    select_cols=[0, 1],\n",
    "    field_delim=\",\",\n",
    "    header=True)\n",
    "\n",
    "sales_test_ds = sales_test_ds.map(map_func=sales_map)\n",
    "# sales_test_ds = sales_test_ds.batch(1)\n",
    "\n",
    "for data in sales_test_ds:\n",
    "    tf.print(\"Test DS sample: \", data)  # {'product_id': [\"P00069042\"], 'user_id': [1000001]}\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ratings DS sample:  {'movie_title': \"One Flew Over the Cuckoo\\'s Nest (1975)\", 'user_id': \"138\"}\n",
    "movies DS sample:  \"You So Crazy (1994)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DS sample:  {'product_id': \"P00128942\", 'user_id': \"1000004\"}\n"
     ]
    }
   ],
   "source": [
    "full_dataset = tf.data.experimental.CsvDataset(\n",
    "    filenames=full_black_friday_ds_path,\n",
    "    # record_defaults=[tf.constant([1000000], dtype=tf.int32), tf.string],\n",
    "    record_defaults=[tf.string, tf.string],\n",
    "    select_cols=[0, 1],\n",
    "    field_delim=\",\",\n",
    "    header=True,\n",
    "\tna_value=\"\")\n",
    "\n",
    "full_dataset = full_dataset.map(map_func=sales_map)\n",
    "# sales_test_ds = sales_test_ds.batch(1)\n",
    "\n",
    "for data in full_dataset:\n",
    "    tf.print(\"Test DS sample: \", data)  # {'product_id': [\"P00069042\"], 'user_id': [1000001]}\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W0HSfmSNCWm"
   },
   "source": [
    "Build vocabularies to convert user ids and movie titles into integer indices for embedding layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make sure that the product_id and user_id are unique - what we have now is immensly slowing down training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9I1VTEjHzpfX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "products dataset sample row:  \"P00069042\"\n",
      "users dataset sample row:  \"1000001\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "products = sales_train_ds.map(lambda x: x[\"product_id\"])\n",
    "products = products.unique()\n",
    "\n",
    "\n",
    "\n",
    "users = sales_train_ds.map(lambda x: x[\"user_id\"])\n",
    "#users = users.unique()\n",
    "\n",
    "for data in products:\n",
    "    tf.print(\"products dataset sample row: \", data)  \n",
    "    break\n",
    "    \n",
    "for data in users:\n",
    "    tf.print(\"users dataset sample row: \", data)  \n",
    "    break    \n",
    "    \n",
    "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(users)\n",
    "#for i in tqdm(range(1)):\n",
    "#    time.sleep(0.01)\n",
    "\n",
    "product_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "product_ids_vocabulary.adapt(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a small dataset to make prototyping faster and easier\n",
    "\n",
    "#small_dataset = sales_train_ds.shuffle()\n",
    "#small_dataset = small_dataset.take(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrch6rVBOB9Q"
   },
   "source": [
    "### Define a model\n",
    "\n",
    "We can define a TFRS model by inheriting from `tfrs.Model` and implementing the `compute_loss` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "e5dNbDZwOIHR"
   },
   "outputs": [],
   "source": [
    "class BlackFridayModel(tfrs.Model):\n",
    "  # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "  # these are still plain Keras Models.\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      user_model: tf.keras.Model,\n",
    "      product_model: tf.keras.Model,\n",
    "      task: tfrs.tasks.Retrieval):\n",
    "    super().__init__()\n",
    "\n",
    "    # Set up user and product representations.\n",
    "    self.user_model = user_model\n",
    "    self.product_model = product_model\n",
    "\n",
    "    # Set up a retrieval task.\n",
    "    self.task = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # Define how the loss is computed.\n",
    "\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    product_embeddings = self.product_model(features[\"product_id\"])\n",
    "\n",
    "    return self.task(user_embeddings, product_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdwtgUCEOI8y"
   },
   "source": [
    "Define the two models and the retrieval task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EvtnUN6aUY4U"
   },
   "outputs": [],
   "source": [
    "# Define user and movie models.\n",
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocabulary_size(), 64)\n",
    "])\n",
    "product_model = tf.keras.Sequential([\n",
    "    product_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(product_ids_vocabulary.vocabulary_size(), 64)\n",
    "])\n",
    "\n",
    "# Define your objectives.\n",
    "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "    products.batch(128).map(product_model)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMV0HpzmJGWk"
   },
   "source": [
    "\n",
    "### Fit and evaluate it.\n",
    "\n",
    "Create the model, train it, and generate predictions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "H2tQDhqkOKf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "135/135 [==============================] - 2519s 19s/step - factorized_top_k/top_1_categorical_accuracy: 8.1808e-05 - factorized_top_k/top_5_categorical_accuracy: 0.0014 - factorized_top_k/top_10_categorical_accuracy: 0.0036 - factorized_top_k/top_50_categorical_accuracy: 0.0251 - factorized_top_k/top_100_categorical_accuracy: 0.0545 - loss: 34125.2507 - regularization_loss: 0.0000e+00 - total_loss: 34125.2507\n",
      "Epoch 2/3\n",
      "135/135 [==============================] - 2551s 19s/step - factorized_top_k/top_1_categorical_accuracy: 2.8906e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0047 - factorized_top_k/top_10_categorical_accuracy: 0.0115 - factorized_top_k/top_50_categorical_accuracy: 0.0696 - factorized_top_k/top_100_categorical_accuracy: 0.1316 - loss: 31977.7145 - regularization_loss: 0.0000e+00 - total_loss: 31977.7145\n",
      "Epoch 3/3\n",
      "135/135 [==============================] - 2563s 19s/step - factorized_top_k/top_1_categorical_accuracy: 6.8901e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0078 - factorized_top_k/top_10_categorical_accuracy: 0.0184 - factorized_top_k/top_50_categorical_accuracy: 0.0971 - factorized_top_k/top_100_categorical_accuracy: 0.1742 - loss: 31338.1793 - regularization_loss: 0.0000e+00 - total_loss: 31338.1793\n",
      "Top 3 recommendations for user 42: [b'P00067542' b'P00272642' b'P00136242']\n"
     ]
    }
   ],
   "source": [
    "# Create a retrieval model.\n",
    "model = BlackFridayModel(user_model, product_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# Setup logs and callback for tensorboard\n",
    "logdir = os.path.join(\"logs_\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "# Train for 3 epochs.\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "model.fit(sales_train_ds.batch(4096), epochs=3, callbacks=[tensorboard_callback])\n",
    "\n",
    "# Use brute-force search to set up retrieval using the trained representations.\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "index.index_from_dataset(\n",
    "    products.batch(100).map(lambda product_id: (product_id, model.product_model(product_id))))\n",
    "\n",
    "# Get some recommendations.\n",
    "_, product_ids = index(np.array([\"42\"]))\n",
    "print(f\"Top 3 recommendations for user 42: {product_ids[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 3555), started 0:11:01 ago. (Use '!kill 3555' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3fd14aa36c2843e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3fd14aa36c2843e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None,) for input KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.string, name='string_lookup_input'), name='string_lookup_input', description=\"created by layer 'string_lookup_input'\"), but it was called on an input with incompatible shape ().\n",
      "WARNING:tensorflow:Model was constructed with shape (None,) for input KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.string, name='string_lookup_1_input'), name='string_lookup_1_input', description=\"created by layer 'string_lookup_1_input'\"), but it was called on an input with incompatible shape ().\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1366, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1356, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1349, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_recommenders/models/base.py\", line 88, in test_step\n        loss = self.compute_loss(inputs, training=False)\n    File \"/tmp/ipykernel_1/1728946185.py\", line 25, in compute_loss\n        return self.task(user_embeddings, product_embeddings)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"retrieval\" (type Retrieval).\n    \n    in user code:\n    \n        File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_recommenders/tasks/retrieval.py\", line 132, in call  *\n            scores = tf.linalg.matmul(\n    \n        ValueError: Shape must be rank 2 but is rank 1 for '{{node retrieval/MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=true](sequential/embedding/embedding_lookup/Identity_1, sequential_1/embedding_1/embedding_lookup/Identity_1)' with input shapes: [64], [64].\n    \n    \n    Call arguments received:\n      • query_embeddings=tf.Tensor(shape=(64,), dtype=float32)\n      • candidate_embeddings=tf.Tensor(shape=(64,), dtype=float32)\n      • sample_weight=None\n      • candidate_sampling_probability=None\n      • candidate_ids=None\n      • compute_metrics=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/2052175059.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msales_test_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1366, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1356, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1349, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_recommenders/models/base.py\", line 88, in test_step\n        loss = self.compute_loss(inputs, training=False)\n    File \"/tmp/ipykernel_1/1728946185.py\", line 25, in compute_loss\n        return self.task(user_embeddings, product_embeddings)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"retrieval\" (type Retrieval).\n    \n    in user code:\n    \n        File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_recommenders/tasks/retrieval.py\", line 132, in call  *\n            scores = tf.linalg.matmul(\n    \n        ValueError: Shape must be rank 2 but is rank 1 for '{{node retrieval/MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=true](sequential/embedding/embedding_lookup/Identity_1, sequential_1/embedding_1/embedding_lookup/Identity_1)' with input shapes: [64], [64].\n    \n    \n    Call arguments received:\n      • query_embeddings=tf.Tensor(shape=(64,), dtype=float32)\n      • candidate_embeddings=tf.Tensor(shape=(64,), dtype=float32)\n      • sample_weight=None\n      • candidate_sampling_probability=None\n      • candidate_ids=None\n      • compute_metrics=True\n"
     ]
    }
   ],
   "source": [
    "# wante\n",
    "model.evaluate(sales_test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 23:35:26.627037: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8or5h_a0/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8or5h_a0/model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations: [b'P00067542' b'P00272642' b'P00136242']\n"
     ]
    }
   ],
   "source": [
    "# Export the query model.\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "  path = os.path.join(tmp, \"model\")\n",
    "\n",
    "  # Save the index.\n",
    "  tf.saved_model.save(index, path)\n",
    "\n",
    "  # Load it back; can also be done in TensorFlow Serving.\n",
    "  loaded = tf.saved_model.load(path)\n",
    "\n",
    "  # Pass a user id in, get top predicted movie titles back.\n",
    "  scores, titles = loaded([\"42\"])\n",
    "\n",
    "  print(f\"Recommendations: {titles[0][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/model1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/model1/assets\n"
     ]
    }
   ],
   "source": [
    "EXPORT_PATH = \"models/model1\"\n",
    "  # Save the index.\n",
    "tf.saved_model.save(index, EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scann_index = tfrs.layers.factorized_top_k.ScaNN(model.user_model)\n",
    "scann_index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations.\n",
    "_, titles = scann_index(tf.constant([\"42\"]))\n",
    "print(f\"Recommendations for user 42: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the query model.\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "  path = os.path.join(tmp, \"model\")\n",
    "\n",
    "  # Save the index.\n",
    "  tf.saved_model.save(\n",
    "      index,\n",
    "      path,\n",
    "      options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"])\n",
    "  )\n",
    "\n",
    "  # Load it back; can also be done in TensorFlow Serving.\n",
    "  loaded = tf.saved_model.load(path)\n",
    "\n",
    "  # Pass a user id in, get top predicted movie titles back.\n",
    "  scores, titles = loaded([\"42\"])\n",
    "\n",
    "  print(f\"Recommendations: {titles[0][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Recommender.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu:latest"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
