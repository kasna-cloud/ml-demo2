{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dEaVsqSgNyQ"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4FyfuZX-gTKS"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sT8AyHRMNh41"
   },
   "source": [
    "# TensorFlow Recommenders: Quickstart\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/quickstart\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f-reQ11gbLB"
   },
   "source": [
    "In this tutorial, we build a simple matrix factorization model using the [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/100k/) with TFRS. We can use this model to recommend movies for a given user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA00wBE2Ntdm"
   },
   "source": [
    "### Import TFRS\n",
    "\n",
    "First, install and import TFRS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6yzAaM85Z12D"
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-recommenders --user\n",
    "!pip install -q --upgrade tensorflow-datasets --user\n",
    "!pip install -q tqdm --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n3oYt3R6Nr9l"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import time    # to be used in loop iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tqmd on jupyter notebook (classic)\n",
    "\n",
    "# !pip install ipywidgets\n",
    "# jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tqmd on jupyter lab\n",
    "jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tmqdm for cell execution progress bar:\n",
    "# The general syntax is just like that. In your loop, just wrap the iterable inside the tqdm()\n",
    "\n",
    "# Loop with a progres bar\n",
    "for i in tqdm(range(100)):\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested loops with progress bar\n",
    "outer_level = list(range(2))\n",
    "inner_level = list(range(100))\n",
    "for _ in tqdm(outer_level, desc='Outer Level'):\n",
    "    for number in tqdm(inner_level, desc='Inner Level'):\n",
    "        time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCxQ1CZcO2wh"
   },
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RcmlxfaPw__0"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>City_Category</th>\n",
       "      <th>Stay_In_Current_City_Years</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Product_Category_1</th>\n",
       "      <th>Product_Category_2</th>\n",
       "      <th>Product_Category_3</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00069042</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00248942</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00087842</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00085442</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000002</td>\n",
       "      <td>P00285442</td>\n",
       "      <td>M</td>\n",
       "      <td>55+</td>\n",
       "      <td>16</td>\n",
       "      <td>C</td>\n",
       "      <td>4+</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Product_ID Gender   Age  Occupation City_Category  \\\n",
       "0  1000001  P00069042      F  0-17          10             A   \n",
       "1  1000001  P00248942      F  0-17          10             A   \n",
       "2  1000001  P00087842      F  0-17          10             A   \n",
       "3  1000001  P00085442      F  0-17          10             A   \n",
       "4  1000002  P00285442      M   55+          16             C   \n",
       "\n",
       "  Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
       "0                          2               0                   3   \n",
       "1                          2               0                   1   \n",
       "2                          2               0                  12   \n",
       "3                          2               0                  12   \n",
       "4                         4+               0                   8   \n",
       "\n",
       "   Product_Category_2  Product_Category_3  Purchase  \n",
       "0                 NaN                 NaN      8370  \n",
       "1                 6.0                14.0     15200  \n",
       "2                 NaN                 NaN      1422  \n",
       "3                14.0                 NaN      1057  \n",
       "4                 NaN                 NaN      7969  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import Black Friday Sales datasets from GCS\n",
    "\n",
    "# connect to gcs\n",
    "client = storage.Client()\n",
    "bucket_name = \"black_friday_datasets\"\n",
    "\n",
    "train_sales_data = \"train_black_friday.csv\"\n",
    "test_sales_data = \"test_black_friday.csv\"\n",
    "concatenated_sales_datasets = \"full_black_friday.csv\"\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# get blobs from bucket\n",
    "blob_train_sale = bucket.get_blob(train_sales_data)\n",
    "blob_test_sale = bucket.get_blob(test_sales_data)\n",
    "blob_concatenated_sales = bucket.get_blob(concatenated_sales_datasets)\n",
    "\n",
    "# save as local files in /datasets\n",
    "filename_train_sale = blob_train_sale.download_to_filename(\"ml-demo2/datasets/\" + train_sales_data)\n",
    "filename_test_sale = blob_test_sale.download_to_filename(\"ml-demo2/datasets/\" + test_sales_data)\n",
    "filename_content_concatenated_sales = blob_concatenated_sales.download_to_filename(\"ml-demo2/datasets/\" + concatenated_sales_datasets)\n",
    "\n",
    "# open one blob to have a look at the data\n",
    "content_train_sale = blob_train_sale.download_as_bytes()\n",
    "\n",
    "black_friday_df = pd.read_csv(BytesIO(content_train_sale))\n",
    "black_friday_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping function\n",
    "def sales_map(user_id, product_id):\n",
    "    return {\"user_id\": user_id, \"product_id\": product_id}\n",
    "\n",
    "\n",
    "# paths to datasets\n",
    "train_csv_path = \"ml-demo2/datasets/train_black_friday.csv\"\n",
    "test_csv_path = \"ml-demo2/datasets/test_black_friday.csv\"\n",
    "full_black_friday_ds_path = \"ml-demo2/datasets/full_black_friday.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert CSV to TensorFlow Datasets and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIuLZsWvxs81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-18 02:28:00.784626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 02:28:01.463569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 02:28:01.464519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 02:28:01.534198: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-18 02:28:01.567796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 02:28:01.568775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 02:28:01.569617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 02:28:13.688616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 02:28:13.689622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 02:28:13.690494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 02:28:13.712004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10806 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DS sample:  {'product_id': \"P00128942\", 'user_id': \"1000004\"}\n",
      "Test DS sample:  {'product_id': \"P00069042\", 'user_id': \"1000001\"}\n"
     ]
    }
   ],
   "source": [
    "# #TODO the model could use more features (currently uses only user_id and product_id)\n",
    "\n",
    "sales_train_ds = tf.data.experimental.CsvDataset(\n",
    "    filenames=train_csv_path,\n",
    "    # record_defaults=[tf.constant([1000000], dtype=tf.int32), tf.string],\n",
    "    record_defaults=[tf.string, tf.string],\n",
    "    select_cols=[0, 1],\n",
    "    field_delim=\",\",\n",
    "    header=True)\n",
    "\n",
    "sales_train_ds = sales_train_ds.map(map_func=sales_map)\n",
    "# sales_train_ds = sales_train_ds.batch(1)\n",
    "\n",
    "for data in sales_train_ds:\n",
    "    tf.print(\"Train DS sample: \", data)  # {'product_id': [\"P00069042\"], 'user_id': [1000001]}\n",
    "    break\n",
    "\n",
    "sales_test_ds = tf.data.experimental.CsvDataset(\n",
    "    filenames=test_csv_path,\n",
    "    # record_defaults=[tf.constant([1000000], dtype=tf.int32), tf.string],\n",
    "    record_defaults=[tf.string, tf.string],\n",
    "    select_cols=[0, 1],\n",
    "    field_delim=\",\",\n",
    "    header=True)\n",
    "\n",
    "sales_test_ds = sales_test_ds.map(map_func=sales_map)\n",
    "# sales_test_ds = sales_test_ds.batch(1)\n",
    "\n",
    "for data in sales_test_ds:\n",
    "    tf.print(\"Test DS sample: \", data)  # {'product_id': [\"P00069042\"], 'user_id': [1000001]}\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DS sample:  {'product_id': \"P00128942\", 'user_id': \"1000004\"}\n"
     ]
    }
   ],
   "source": [
    "full_dataset = tf.data.experimental.CsvDataset(\n",
    "    filenames=full_black_friday_ds_path,\n",
    "    # record_defaults=[tf.constant([1000000], dtype=tf.int32), tf.string],\n",
    "    record_defaults=[tf.string, tf.string],\n",
    "    select_cols=[0, 1],\n",
    "    field_delim=\",\",\n",
    "    header=True,\n",
    "\tna_value=\"\")\n",
    "\n",
    "full_dataset = full_dataset.map(map_func=sales_map)\n",
    "# sales_test_ds = sales_test_ds.batch(1)\n",
    "\n",
    "for data in full_dataset:\n",
    "    tf.print(\"Test DS sample: \", data)  # {'product_id': [\"P00069042\"], 'user_id': [1000001]}\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W0HSfmSNCWm"
   },
   "source": [
    "Build vocabularies to convert user ids and movie titles into integer indices for embedding layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make sure that the product_id and user_id are unique - what we have now is immensly slowing down training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9I1VTEjHzpfX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full DS sample:  {'product_id': \"P00128942\", 'user_id': \"1000004\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "products = full_dataset.map(lambda x: x[\"product_id\"])\n",
    "\n",
    "for data in full_dataset:\n",
    "    tf.print(\"full DS sample: \", data)  # {'product_id': [\"P00069042\"], 'user_id': [1000001]}\n",
    "    break\n",
    "    \n",
    "    \n",
    "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(full_dataset.map(lambda x: x[\"user_id\"]))\n",
    "\n",
    "product_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "product_ids_vocabulary.adapt(full_dataset.map(lambda x: x[\"product_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a small dataset to make prototyping faster and easier\n",
    "\n",
    "small_dataset = sales_train_ds.shuffle()\n",
    "small_dataset = small_dataset.take(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrch6rVBOB9Q"
   },
   "source": [
    "### Define a model\n",
    "\n",
    "We can define a TFRS model by inheriting from `tfrs.Model` and implementing the `compute_loss` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "e5dNbDZwOIHR"
   },
   "outputs": [],
   "source": [
    "class BlackFridayModel(tfrs.Model):\n",
    "  # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "  # these are still plain Keras Models.\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      user_model: tf.keras.Model,\n",
    "      product_model: tf.keras.Model,\n",
    "      task: tfrs.tasks.Retrieval):\n",
    "    super().__init__()\n",
    "\n",
    "    # Set up user and product representations.\n",
    "    self.user_model = user_model\n",
    "    self.product_model = product_model\n",
    "\n",
    "    # Set up a retrieval task.\n",
    "    self.task = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # Define how the loss is computed.\n",
    "\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    product_embeddings = self.product_model(features[\"product_id\"])\n",
    "\n",
    "    return self.task(user_embeddings, product_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdwtgUCEOI8y"
   },
   "source": [
    "Define the two models and the retrieval task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EvtnUN6aUY4U"
   },
   "outputs": [],
   "source": [
    "# Define user and movie models.\n",
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocabulary_size(), 64)\n",
    "])\n",
    "product_model = tf.keras.Sequential([\n",
    "    product_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(product_ids_vocabulary.vocabulary_size(), 64)\n",
    "])\n",
    "\n",
    "# Define your objectives.\n",
    "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "    products.batch(128).map(product_model)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMV0HpzmJGWk"
   },
   "source": [
    "\n",
    "### Fit and evaluate it.\n",
    "\n",
    "Create the model, train it, and generate predictions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2tQDhqkOKf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "229/229 [==============================] - 3672s 16s/step - factorized_top_k/top_1_categorical_accuracy: 8.5617e-06 - factorized_top_k/top_5_categorical_accuracy: 8.5617e-06 - factorized_top_k/top_10_categorical_accuracy: 8.5617e-06 - factorized_top_k/top_50_categorical_accuracy: 8.9898e-05 - factorized_top_k/top_100_categorical_accuracy: 1.9264e-04 - loss: 7365.0951 - regularization_loss: 0.0000e+00 - total_loss: 7365.0951\n",
      "Epoch 2/25\n",
      "229/229 [==============================] - 3607s 16s/step - factorized_top_k/top_1_categorical_accuracy: 2.0120e-04 - factorized_top_k/top_5_categorical_accuracy: 7.0634e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0011 - factorized_top_k/top_50_categorical_accuracy: 0.0038 - factorized_top_k/top_100_categorical_accuracy: 0.0061 - loss: 6603.7103 - regularization_loss: 0.0000e+00 - total_loss: 6603.7103\n",
      "Epoch 3/25\n",
      "229/229 [==============================] - 3558s 16s/step - factorized_top_k/top_1_categorical_accuracy: 0.0014 - factorized_top_k/top_5_categorical_accuracy: 0.0023 - factorized_top_k/top_10_categorical_accuracy: 0.0033 - factorized_top_k/top_50_categorical_accuracy: 0.0083 - factorized_top_k/top_100_categorical_accuracy: 0.0122 - loss: 6297.4909 - regularization_loss: 0.0000e+00 - total_loss: 6297.4909\n",
      "Epoch 4/25\n",
      "229/229 [==============================] - 3564s 16s/step - factorized_top_k/top_1_categorical_accuracy: 8.9041e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0020 - factorized_top_k/top_10_categorical_accuracy: 0.0033 - factorized_top_k/top_50_categorical_accuracy: 0.0098 - factorized_top_k/top_100_categorical_accuracy: 0.0152 - loss: 6101.5700 - regularization_loss: 0.0000e+00 - total_loss: 6101.5700\n",
      "Epoch 5/25\n",
      "229/229 [==============================] - 3642s 16s/step - factorized_top_k/top_1_categorical_accuracy: 0.0013 - factorized_top_k/top_5_categorical_accuracy: 0.0025 - factorized_top_k/top_10_categorical_accuracy: 0.0039 - factorized_top_k/top_50_categorical_accuracy: 0.0113 - factorized_top_k/top_100_categorical_accuracy: 0.0174 - loss: 5961.1718 - regularization_loss: 0.0000e+00 - total_loss: 5961.1718\n",
      "Epoch 6/25\n",
      "229/229 [==============================] - 3696s 16s/step - factorized_top_k/top_1_categorical_accuracy: 0.0013 - factorized_top_k/top_5_categorical_accuracy: 0.0026 - factorized_top_k/top_10_categorical_accuracy: 0.0041 - factorized_top_k/top_50_categorical_accuracy: 0.0121 - factorized_top_k/top_100_categorical_accuracy: 0.0188 - loss: 5856.5211 - regularization_loss: 0.0000e+00 - total_loss: 5856.5211\n",
      "Epoch 7/25\n",
      " 63/229 [=======>......................] - ETA: 44:42 - factorized_top_k/top_1_categorical_accuracy: 0.0012 - factorized_top_k/top_5_categorical_accuracy: 0.0027 - factorized_top_k/top_10_categorical_accuracy: 0.0042 - factorized_top_k/top_50_categorical_accuracy: 0.0127 - factorized_top_k/top_100_categorical_accuracy: 0.0193 - loss: 5856.0292 - regularization_loss: 0.0000e+00 - total_loss: 5856.0292"
     ]
    }
   ],
   "source": [
    "# Create a retrieval model.\n",
    "model = BlackFridayModel(user_model, product_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# Train for 3 epochs.\n",
    "model.fit(full_dataset.batch(1024), epochs=25)\n",
    "\n",
    "# Use brute-force search to set up retrieval using the trained representations.\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "index.index_from_dataset(\n",
    "    products.batch(100).map(lambda product_id: (product_id, model.product_model(product_id))))\n",
    "\n",
    "# Get some recommendations.\n",
    "_, product_ids = index(np.array([\"42\"]))\n",
    "print(f\"Top 3 recommendations for user 42: {product_ids[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neJAJVwbReNd"
   },
   "outputs": [],
   "source": [
    "# 4. Save model\n",
    "EXPORT_PATH = \"modesl/model_1\"\n",
    "model.save(EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Recommender.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu:latest"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
